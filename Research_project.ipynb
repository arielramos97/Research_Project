{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8xtAB1MX84i"
      },
      "source": [
        "# Research Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ur36uWwX84k"
      },
      "source": [
        "## Import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRSiuSJiYDDa",
        "outputId": "7cfdbffd-b054-4e8a-8a8f-f2ad5e8ad154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.22.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.15.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: bert_score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.5.3)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.30.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (23.1)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2022.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert_score) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.15.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.3.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.0.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.2.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=3.0.0->bert_score) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert_score) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge_score bert_score sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HTapU5AX84l"
      },
      "outputs": [],
      "source": [
        "from datasets import list_datasets, load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2Tokenizer, GPT2LMHeadModel, get_polynomial_decay_schedule_with_warmup\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from itertools import chain\n",
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "import datasets\n",
        "\n",
        "#For networking purposes\n",
        "import os, sys\n",
        "os.environ['CURL_CA_BUNDLE'] = ''\n",
        "\n",
        "import urllib3\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsmMWLe9mM3u"
      },
      "outputs": [],
      "source": [
        "selected_model = 'dialoGPT' #''gpt2' \n",
        "dataset_name = 'empathetic_dialogues'#'daily_dialog' "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOExHyilX84m"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQXJ6YGcX84m"
      },
      "outputs": [],
      "source": [
        "if selected_model == 'dialoGPT':\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "elif selected_model == 'gpt2':\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "else:\n",
        "    print('No tokenizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6wliEy_DX84n"
      },
      "outputs": [],
      "source": [
        "space = 'Ġ'\n",
        "pre_quote = '’'\n",
        "end_marks = ['.', ',', '?', '!', '...']\n",
        "quotes = ['\"', '\\'']\n",
        "abbreviations = ['s', 'd', 't', 'm', 're', 'll', 've', 'S', 'D', 'T', 'M', 'Re', 'Ll', 'Ve']\n",
        "\n",
        "# For empathetic dialogues\n",
        "exclude_symbol = \"_conv\"\n",
        "comma_symbol = \"_comma_\"\n",
        "\n",
        "def process_token_list(token_list):\n",
        "    token_list[0] = token_list[0].capitalize()\n",
        "    \n",
        "    quote_count = 0\n",
        "    for i, token in enumerate(token_list):\n",
        "        if space in token:\n",
        "            if token[1:] in end_marks or token[1:] in abbreviations:\n",
        "                token_list[i] = token[1:]\n",
        "                \n",
        "            if token[1:] == quotes[1]:\n",
        "                if i<len(token_list)-1:\n",
        "                    if token_list[i+1] in abbreviations or (token_list[i+1][0] == space and token_list[i+1][1:] in abbreviations):\n",
        "                        token_list[i] = token[1:]\n",
        "                        \n",
        "        if token[0] == space and token[1:] in quotes:\n",
        "            if quote_count % 2 == 1:\n",
        "                token_list[i] = token[1:]\n",
        "                quote_count = 0\n",
        "            else:\n",
        "                if i<len(token_list)-1 and token_list[i+1][0] == space:\n",
        "                    token_list[i+1] = token_list[i+1][1:]\n",
        "                quote_count += 1\n",
        "                \n",
        "        if token in end_marks or token[1:] in end_marks:\n",
        "            if i<len(token_list)-1:\n",
        "                if token_list[i+1][0] != space:\n",
        "                    token_list[i+1] = space + token_list[i+1].capitalize()\n",
        "                else:\n",
        "                    token_list[i+1] = space + token_list[i+1][1:].capitalize()\n",
        "                \n",
        "    new_token_list = [token for token in token_list if token != space and len(token)>0]\n",
        "    if new_token_list[-1] not in end_marks:\n",
        "        new_token_list.append(end_marks[0])\n",
        "        \n",
        "    return new_token_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAtmvBqXwkr8"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "f664fd67577746e08255ecbdce4d7414",
            "568c416462e94684978348f5221d452c",
            "edb5a3f280a243019cffc86268a70226",
            "7a3a3a1274504f86a1197b0431d2f1a9",
            "f3901da562094eab903947ab3932e78e",
            "c7f7f9e86afc487bad4225675751c2d9",
            "941c2437df2642358d2ae8346dd29fe1",
            "f33d90c071ef472d88da1f82e99b2767",
            "c537cd567f6c4004852d89f2d958b13b",
            "9e54e40923eb40bfa198cfa51a61118c",
            "5d7bad610ef64a14889724b74af36d65"
          ]
        },
        "id": "KbddkkMRbxwA",
        "outputId": "43ee52ce-746c-41b6-d75b-46dbd1e1c265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading  empathetic_dialogues\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset empathetic_dialogues (/root/.cache/huggingface/datasets/empathetic_dialogues/default/0.1.0/09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f664fd67577746e08255ecbdce4d7414"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if dataset_name == 'daily_dialog':\n",
        "    print('Loading ', dataset_name)\n",
        "    dataset = load_dataset('daily_dialog')\n",
        "    train_dialogues = dataset['train']['dialog']\n",
        "    valid_dialogues = dataset['validation']['dialog']\n",
        "    test_dialogues = dataset['test']['dialog']\n",
        "elif dataset_name == 'empathetic_dialogues':\n",
        "    print('Loading ', dataset_name)\n",
        "    dataset = load_dataset('empathetic_dialogues')\n",
        "    train_dialogues = dataset['train']\n",
        "    valid_dialogues = dataset['validation']\n",
        "    test_dialogues = dataset['test']\n",
        "else:\n",
        "    print('No dataset selected')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeR1rn_XX84o"
      },
      "outputs": [],
      "source": [
        "def load_empathetic(dataset, tokenizer):\n",
        "    \n",
        "    total_utters = dataset['utterance']\n",
        "    total_conv_ids = dataset['conv_id'] \n",
        "    total_speaker_ids = dataset['speaker_idx']\n",
        "    \n",
        "    assert len(total_utters) == len(total_conv_ids) and len(total_conv_ids) == len(total_speaker_ids)\n",
        "    \n",
        "    num = 0\n",
        "    \n",
        "    conv_dict = {}\n",
        "    cur_speaker_idx = -1\n",
        "    for i, utter in enumerate(tqdm(total_utters)):\n",
        "        conv_id = total_conv_ids[i]\n",
        "        speaker_idx = total_speaker_ids[i]\n",
        "        \n",
        "        utter_modified = utter.strip().replace(comma_symbol, ',')\n",
        "        new_token_list = process_token_list(tokenizer.tokenize(utter_modified))\n",
        "        text = tokenizer.convert_tokens_to_string(new_token_list)\n",
        "        \n",
        "        if exclude_symbol in utter:\n",
        "            continue\n",
        "        \n",
        "        if conv_id not in conv_dict:\n",
        "            conv_dict[conv_id] = []\n",
        "            cur_speaker_idx = -1\n",
        "\n",
        "        if cur_speaker_idx != speaker_idx:\n",
        "            conv_dict[conv_id].append(text)\n",
        "            cur_speaker_idx = speaker_idx\n",
        "        else:\n",
        "            conv_dict[conv_id][-1] += f\" {text}\"\n",
        "    \n",
        "    utter_num = 0\n",
        "    dialogues = []\n",
        "    \n",
        "    for i, (conv_id, utter_list) in enumerate(conv_dict.items()):\n",
        "        utter_num += len(utter_list)\n",
        "        dialogues.append(utter_list)\n",
        "            \n",
        "    return dialogues, utter_num\n",
        "\n",
        "def load_daily(dataset, tokenizer):\n",
        "        \n",
        "    for i, dialogue in enumerate(tqdm(dataset)):\n",
        "        new_dialogue = []\n",
        "        for utter in dialogue:\n",
        "            token_list = tokenizer.tokenize(utter.strip().replace(pre_quote, quotes[1]))\n",
        "            token_list = process_token_list(token_list)\n",
        "            text = tokenizer.convert_tokens_to_string(token_list)\n",
        "            new_dialogue.append(text)\n",
        "            \n",
        "        dataset[i] = new_dialogue\n",
        "    \n",
        "    utter_num = 0\n",
        "\n",
        "    for dialogue in dataset:\n",
        "        utter_num += len(dialogue)\n",
        "    \n",
        "    return dataset, utter_num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G_Xjm7HdCg4",
        "outputId": "7235ca28-dfab-4031-efe9-f9c1d2992f5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 1443/76673 [00:00<00:10, 7247.66it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (6234 > 1024). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 76673/76673 [00:11<00:00, 6916.31it/s]\n",
            "100%|██████████| 12030/12030 [00:02<00:00, 5088.97it/s]\n",
            "100%|██████████| 10943/10943 [00:02<00:00, 4032.50it/s]\n"
          ]
        }
      ],
      "source": [
        "if dataset_name == 'daily_dialog':\n",
        "    train_dialogues, num_train = load_daily(train_dialogues, tokenizer)\n",
        "    valid_dialogues, num_valid = load_daily(valid_dialogues, tokenizer)\n",
        "    test_dialogues, num_test = load_daily(test_dialogues, tokenizer)\n",
        "elif dataset_name == 'empathetic_dialogues':\n",
        "    train_dialogues, num_train = load_empathetic(train_dialogues, tokenizer)\n",
        "    valid_dialogues, num_valid = load_empathetic(valid_dialogues, tokenizer)\n",
        "    test_dialogues, num_test = load_empathetic(test_dialogues, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbQnJhewX84o",
        "outputId": "649bfc4d-0bc0-4167-c2dc-6d6ce917ff41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of train dialogues: 17793\n",
            "The number of valid dialogues: 2759\n",
            "The number of test dialogues: 2540\n",
            "The number of train utterances: 76622\n",
            "The number of valid utterances: 12025\n",
            "The number of test utterances: 10939\n"
          ]
        }
      ],
      "source": [
        "print(f\"The number of train dialogues: {len(train_dialogues)}\")\n",
        "print(f\"The number of valid dialogues: {len(valid_dialogues)}\")    \n",
        "print(f\"The number of test dialogues: {len(test_dialogues)}\")    \n",
        "\n",
        "print(f\"The number of train utterances: {num_train}\")    \n",
        "print(f\"The number of valid utterances: {num_valid}\")\n",
        "print(f\"The number of test utterances: {num_test}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHOC7JZKX84p"
      },
      "outputs": [],
      "source": [
        "# Extrac ids (input_ids, token_ids) from processed text\n",
        "def extract_ids(dialogues):\n",
        "\n",
        "    ids = []\n",
        "    for dialogue in tqdm(dialogues):\n",
        "            dialogue_ids = []\n",
        "            for utter in dialogue:\n",
        "                tokens = tokenizer.tokenize(utter)\n",
        "                token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "                dialogue_ids.append(token_ids)\n",
        "            ids.append(dialogue_ids)\n",
        "            \n",
        "    assert len(ids) == len(dialogues)\n",
        "\n",
        "    return ids  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdpY87Y3pa1R",
        "outputId": "7fbe1b0e-573b-40aa-ff21-02022b6bc998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17793/17793 [00:09<00:00, 1817.94it/s]\n",
            "100%|██████████| 2759/2759 [00:02<00:00, 1313.96it/s]\n"
          ]
        }
      ],
      "source": [
        "train_ids = extract_ids(train_dialogues)\n",
        "valid_ids = extract_ids(valid_dialogues) \n",
        "# test_ids = extract_ids(test_dialogues) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1GL9sQlX84p"
      },
      "outputs": [],
      "source": [
        "#Parameters\n",
        "\n",
        "sp1_token = '<sp1>'\n",
        "sp2_token = '<sp2>'\n",
        "bos_token = '<bos>'\n",
        "max_turns = 5\n",
        "max_len = 1024\n",
        "seed = 0\n",
        "gpu = 0\n",
        "\n",
        "#Tokeniser\n",
        "special_tokens = {'bos_token': bos_token,\n",
        "                'additional_special_tokens': [sp1_token, sp2_token]}\n",
        "\n",
        "eos_token = tokenizer.eos_token\n",
        "num_new_tokens = tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "vocab = tokenizer.get_vocab()\n",
        "vocab_size = len(vocab)\n",
        "bos_id = vocab[bos_token]\n",
        "eos_id = vocab[eos_token]\n",
        "sp1_id = vocab[sp1_token]\n",
        "sp2_id = vocab[sp2_token]\n",
        "\n",
        "lr = 2e-5\n",
        "batch_size = 8\n",
        "num_workers = 0\n",
        "num_epochs = 8\n",
        "warmup_ratio = 0.1\n",
        "last_epoch = 0\n",
        "end_command = 'Quit!'\n",
        "top_p = 0.8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VykNu-MqqSr"
      },
      "outputs": [],
      "source": [
        "!mkdir 'saved_models'\n",
        "ckpt_dir = 'saved_models'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlfw1N6lX84q"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dials):\n",
        "\n",
        "        self.input_ids = []  # (N, L)\n",
        "        self.token_type_ids = []  # (N, L)\n",
        "        self.labels = []  # (N, L)\n",
        "            \n",
        "        for dial in tqdm(dials):\n",
        "            hists = []\n",
        "            for u, utter in enumerate(dial):\n",
        "                if u % 2 == 0:\n",
        "                    hists.append([sp1_id] + utter)\n",
        "                else:\n",
        "                    hists.append([sp2_id] + utter)\n",
        "\n",
        "            # print(hists) \n",
        "            # print() \n",
        "            for h in range(len(hists)):\n",
        "                if hists[h][0] == sp2_id:\n",
        "                    # print(hists[h])\n",
        "                    start = max(0, h - max_turns+1)\n",
        "                    # print('start: ', start, ' to: ', h)\n",
        "                    for s in range(start, h):\n",
        "                        contexts = hists[s:h+1]\n",
        "                        # print('Context: ', contexts)\n",
        "                        input_ids = [bos_id] + list(chain.from_iterable(contexts)) + [eos_id]\n",
        "                        if len(input_ids) <= max_len:\n",
        "                            start_sp_id, next_sp_id = contexts[0][0], contexts[1][0]\n",
        "                            token_type_ids = [[start_sp_id] * len(ctx) if c % 2 == 0 else [next_sp_id] * len(ctx) for c, ctx in enumerate(contexts)]\n",
        "                            # print('token_type_ids 1: ', token_type_ids)\n",
        "                            # print('LEN 1: ', len(token_type_ids))\n",
        "                            # print('len input_ids', len(input_ids))\n",
        "                            assert token_type_ids[-1][0] == sp2_id\n",
        "                            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [sp2_id]\n",
        "                            # print('token_type_ids 2: ', token_type_ids)\n",
        "                            assert len(input_ids) == len(token_type_ids)\n",
        "                            \n",
        "                            labels = [[-100] * len(ctx) if c < len(contexts)-1 else [-100] + ctx[1:] for c, ctx in enumerate(contexts)]\n",
        "                            # print('labels 1: ', labels)\n",
        "                            assert labels[-1][1:] == contexts[-1][1:]\n",
        "                            labels = [-100] + list(chain.from_iterable(labels)) + [eos_id]\n",
        "                            # print('labels 2: ', labels)\n",
        "                            assert len(input_ids) == len(labels)\n",
        "                            \n",
        "                            self.input_ids.append(input_ids)\n",
        "                            self.token_type_ids.append(token_type_ids)\n",
        "                            self.labels.append(labels)\n",
        "                            \n",
        "                            break\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.token_type_ids[idx], self.labels[idx]\n",
        "    \n",
        "    \n",
        "class PadCollate():\n",
        "    def __init__(self, eos_id):\n",
        "        self.eos_id = eos_id\n",
        "        \n",
        "    def pad_collate(self, batch):\n",
        "        input_ids, token_type_ids, labels =[], [], []\n",
        "        for idx, seqs in enumerate(batch):\n",
        "            input_ids.append(torch.LongTensor(seqs[0]))\n",
        "            token_type_ids.append(torch.LongTensor(seqs[1]))\n",
        "            labels.append(torch.LongTensor(seqs[2]))\n",
        "            \n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.eos_id)\n",
        "        token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids, batch_first=True, padding_value=self.eos_id)\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
        "    \n",
        "        return input_ids, token_type_ids, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqxf8rTWft_V",
        "outputId": "e5145e1f-3113-4828-b1e5-d265534e9aeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50256 50257 50258 50259\n"
          ]
        }
      ],
      "source": [
        "print(eos_id, bos_id, sp1_id, sp2_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62axctEhtAbM",
        "outputId": "729ae83c-234c-400d-95a2-774dd4e461e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I remember going to see the fireworks with my best friend. It was the first time we ever spent time alone together. Although there was a lot of people, We felt like the only people in the world.', 'Was this a friend you were in love with, Or just a best friend?', 'This was a best friend. I miss her.', 'Where has she gone?', 'We no longer talk.', 'Oh was this something that happened because of an argument?']\n"
          ]
        }
      ],
      "source": [
        "print(train_dialogues[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9qPnWf6s4yf",
        "outputId": "e31a7e2b-40bb-4139-bdb6-21d57c92d62c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 4194.30it/s]\n"
          ]
        }
      ],
      "source": [
        "#Debugging purposes\n",
        "debug_dialog = CustomDataset([train_ids[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDqn54G2X84q"
      },
      "outputs": [],
      "source": [
        "def fix_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDO8Z1tIUdbq",
        "outputId": "a0b2d635-ebe6-4c76-d108-a08a444aad77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        }
      ],
      "source": [
        "#Load Model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(f\"cuda:{gpu}\")\n",
        "    print('Using GPU')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('Using CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYEsMTDCUq8f",
        "outputId": "0050a711-7a22-41b5-bdce-e55e08ba62aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the model:  dialoGPT\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading the model: \", selected_model)\n",
        "fix_seed(seed)\n",
        "\n",
        "if selected_model == 'dialoGPT':\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\").to(device)\n",
        "elif selected_model == 'gpt2':\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
        "else:\n",
        "    print('No model')\n",
        "\n",
        "model.resize_token_embeddings(vocab_size)\n",
        "max_len = min(max_len, model.config.n_ctx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZTG55SkT9A1",
        "outputId": "4a9f2876-4f2b-4fb0-d917-ec78c0619515"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#Load from checkpoint\n",
        "ckpt = torch.load(\"/content/saved_models/best_ckpt_epoch=4_valid_loss=2.5344.ckpt\", map_location=device)\n",
        "model.load_state_dict(ckpt['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZdgaMdyini_",
        "outputId": "503028fe-a73f-46a7-ce65-fef32bede6d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading the optimizer...\n"
          ]
        }
      ],
      "source": [
        "# Load optimizer\n",
        "print(\"Loading the optimizer...\")\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yiJtEiYX84r",
        "outputId": "e364b3cb-b90f-43d9-e014-df8a7314cca2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train & valid data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17793/17793 [00:00<00:00, 23742.94it/s]\n",
            "100%|██████████| 2759/2759 [00:00<00:00, 40248.91it/s]\n"
          ]
        }
      ],
      "source": [
        "# Load train & valid dataset\n",
        "print(\"Loading train & valid data...\")\n",
        "\n",
        "train_set = CustomDataset(train_ids)\n",
        "valid_set = CustomDataset(valid_ids)\n",
        "# test_set = CustomDataset(test_ids)\n",
        "\n",
        "ppd = PadCollate(eos_id=eos_id)\n",
        "\n",
        "train_loader = DataLoader(train_set, \n",
        "                            collate_fn=ppd.pad_collate, \n",
        "                            shuffle=True, \n",
        "                            batch_size=batch_size, \n",
        "                            num_workers=num_workers, \n",
        "                            pin_memory=True)\n",
        "\n",
        "valid_loader = DataLoader(valid_set, \n",
        "                            collate_fn=ppd.pad_collate,\n",
        "                            batch_size=batch_size, \n",
        "                            num_workers=num_workers, \n",
        "                            pin_memory=True)\n",
        "\n",
        "# test_loader = DataLoader(test_set, \n",
        "#                             collate_fn=ppd.pad_collate,\n",
        "#                             batch_size=batch_size, \n",
        "#                             num_workers=num_workers, \n",
        "#                             pin_memory=True)\n",
        "    \n",
        "# Calculate total training steps\n",
        "num_batches = len(train_loader)\n",
        "total_train_steps = num_epochs * num_batches\n",
        "warmup_steps = int(warmup_ratio * total_train_steps)\n",
        "\n",
        "sched = get_polynomial_decay_schedule_with_warmup(\n",
        "    optim,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_train_steps,\n",
        "    power=2\n",
        ")\n",
        "\n",
        "writer = SummaryWriter()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRsAHWtUX84r"
      },
      "outputs": [],
      "source": [
        "def validation():\n",
        "\n",
        "    print(\"Validation processing...\")\n",
        "    model.eval()\n",
        "            \n",
        "    valid_losses = []\n",
        "    valid_ppls = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(tqdm(valid_loader)):\n",
        "            input_ids, token_type_ids, labels = batch\n",
        "            input_ids, token_type_ids, labels = \\\n",
        "                input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                token_type_ids = token_type_ids,\n",
        "                labels = labels\n",
        "            )\n",
        "            \n",
        "            loss, logits = outputs[0], outputs[1]\n",
        "            \n",
        "            valid_losses.append(loss.detach())\n",
        "            ppl = torch.exp(loss.detach())\n",
        "            valid_ppls.append(ppl)\n",
        "        \n",
        "        valid_losses = [loss.item() for loss in valid_losses]\n",
        "        valid_ppls = [ppl.item() if not math.isinf(ppl.item()) else 1e+8 for ppl in valid_ppls]\n",
        "        valid_loss = np.mean(valid_losses)\n",
        "        valid_ppl = np.mean(valid_ppls)\n",
        "        \n",
        "        if math.isnan(valid_ppl):\n",
        "            valid_ppl = 1e+8\n",
        "            \n",
        "    return valid_loss, valid_ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBYF0EyJX84x"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    \n",
        "    fix_seed(seed)  # Fix seed before training\n",
        "    print(\"Training starts.\")\n",
        "\n",
        "    best_loss = sys.float_info.max\n",
        "    last_epoch= 0\n",
        "    \n",
        "    start_epoch = last_epoch +1\n",
        "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
        "        model.train()\n",
        "        \n",
        "        print(f\"#\"*50 + f\"Epoch: {epoch}\" + \"#\"*50)\n",
        "        train_losses = []\n",
        "        train_ppls = []\n",
        "        for i, batch in enumerate(tqdm(train_loader)):\n",
        "            input_ids, token_type_ids, labels = batch\n",
        "            input_ids, token_type_ids, labels = \\\n",
        "                input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
        "            \n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                token_type_ids = token_type_ids,\n",
        "                labels = labels\n",
        "            )\n",
        "            \n",
        "            loss, logits = outputs[0], outputs[1]\n",
        "            \n",
        "            optim.zero_grad()\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            sched.step()\n",
        "            \n",
        "            train_losses.append(loss.detach())\n",
        "            ppl = torch.exp(loss.detach())\n",
        "            train_ppls.append(ppl)\n",
        "        \n",
        "        train_losses = [loss.item() for loss in train_losses]\n",
        "        train_ppls = [ppl.item() if not math.isinf(ppl.item()) else 1e+8 for ppl in train_ppls]\n",
        "        train_loss = np.mean(train_losses)\n",
        "        train_ppl = np.mean(train_ppls)\n",
        "        print(f\"Train loss: {train_loss} || Train perplexity: {train_ppl}\")\n",
        "        \n",
        "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "        writer.add_scalar(\"PPL/train\", train_ppl, epoch)\n",
        "        \n",
        "        last_epoch += 1\n",
        "        \n",
        "        valid_loss, valid_ppl = validation()\n",
        "            \n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            state_dict = {\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optim_state_dict': optim.state_dict(),\n",
        "                'sched_state_dict': sched.state_dict(),\n",
        "                'loss': best_loss,\n",
        "                'epoch': last_epoch\n",
        "            }\n",
        "            \n",
        "            torch.save(state_dict, f\"{ckpt_dir}/best_ckpt_epoch={epoch}_valid_loss={round(best_loss, 4)}.ckpt\")\n",
        "            print(\"*\"*10 + \"Current best checkpoint is saved.\" + \"*\"*10)\n",
        "            print(f\"{ckpt_dir}/best_ckpt_epoch={epoch}_valid_loss={round(best_loss, 4)}.ckpt\")\n",
        "            \n",
        "        print(f\"Best valid loss: {best_loss}\")\n",
        "        print(f\"Valid loss: {valid_loss} || Valid perplexity: {valid_ppl}\")\n",
        "        \n",
        "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
        "        writer.add_scalar(\"PPL/valid\", valid_ppl, epoch)\n",
        "        \n",
        "        writer.add_scalars(\"Losses\", {\n",
        "            'train': train_loss, \n",
        "            'valid': valid_loss,\n",
        "        }, epoch)\n",
        "        writer.add_scalars(\"PPLs\", {\n",
        "            'train': train_ppl,\n",
        "            'valid': valid_ppl,\n",
        "        }, epoch)\n",
        "            \n",
        "    print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIpXVxEdX84x",
        "outputId": "2037af26-b185-4c4d-8ba8-11f80f9601bf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training starts.\n",
            "##################################################Epoch: 1##################################################\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4579/4579 [19:40<00:00,  3.88it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 3.4754341883960316 || Train perplexity: 136.05326380107985\n",
            "Validation processing...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.84it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********Current best checkpoint is saved.**********\n",
            "saved_models/best_ckpt_epoch=1_valid_loss=2.6207.ckpt\n",
            "Best valid loss: 2.6207386190149964\n",
            "Valid loss: 2.6207386190149964 || Valid perplexity: 14.652000994909377\n",
            "##################################################Epoch: 2##################################################\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4579/4579 [19:47<00:00,  3.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 2.6777115453406437 || Train perplexity: 15.279803808507609\n",
            "Validation processing...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.88it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********Current best checkpoint is saved.**********\n",
            "saved_models/best_ckpt_epoch=2_valid_loss=2.5557.ckpt\n",
            "Best valid loss: 2.5557342567363706\n",
            "Valid loss: 2.5557342567363706 || Valid perplexity: 13.708324942602163\n",
            "##################################################Epoch: 3##################################################\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4579/4579 [19:46<00:00,  3.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 2.524160280111029 || Train perplexity: 13.014129212652916\n",
            "Validation processing...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.86it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**********Current best checkpoint is saved.**********\n",
            "saved_models/best_ckpt_epoch=3_valid_loss=2.5367.ckpt\n",
            "Best valid loss: 2.5366925942797622\n",
            "Valid loss: 2.5366925942797622 || Valid perplexity: 13.476058622702164\n",
            "##################################################Epoch: 4##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4579/4579 [19:44<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.4330872132492316 || Train perplexity: 11.846613157521848\n",
            "Validation processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********Current best checkpoint is saved.**********\n",
            "saved_models/best_ckpt_epoch=4_valid_loss=2.5344.ckpt\n",
            "Best valid loss: 2.5344023479133093\n",
            "Valid loss: 2.5344023479133093 || Valid perplexity: 13.472681053546296\n",
            "##################################################Epoch: 5##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4579/4579 [19:45<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.3745743261723833 || Train perplexity: 11.147166979211365\n",
            "Validation processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best valid loss: 2.5344023479133093\n",
            "Valid loss: 2.5389193782285484 || Valid perplexity: 13.566308946502643\n",
            "##################################################Epoch: 6##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4579/4579 [19:49<00:00,  3.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.3399209027780823 || Train perplexity: 10.764859713433376\n",
            "Validation processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best valid loss: 2.5344023479133093\n",
            "Valid loss: 2.543360740363765 || Valid perplexity: 13.648788542974563\n",
            "##################################################Epoch: 7##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4579/4579 [19:47<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.3189223773303995 || Train perplexity: 10.523445773057007\n",
            "Validation processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best valid loss: 2.5344023479133093\n",
            "Valid loss: 2.5462398779492417 || Valid perplexity: 13.700725489303846\n",
            "##################################################Epoch: 8##################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4579/4579 [19:47<00:00,  3.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.309843450999463 || Train perplexity: 10.428232942457464\n",
            "Validation processing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 714/714 [00:55<00:00, 12.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best valid loss: 2.5344023479133093\n",
            "Valid loss: 2.546788294275268 || Valid perplexity: 13.710986280307717\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRCITuBxX84x"
      },
      "outputs": [],
      "source": [
        "window = 5\n",
        "\n",
        "def infer():\n",
        "    model.eval()\n",
        "    fix_seed(seed)\n",
        "\n",
        "    generated_responses = []\n",
        "    actual_responses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        for i, batch in enumerate(tqdm(test_dialogues)):\n",
        "\n",
        "            # print()\n",
        "            # print(batch)\n",
        "            input_hists = []\n",
        "            context = []\n",
        "            for j in range(0, len(batch)): #Note: dialogues < window won't be processed\n",
        "\n",
        "                #Set speaker 1 or speaker 2\n",
        "                sp_id = sp1_id if j % 2 == 0 else sp2_id\n",
        "\n",
        "                #Get utterance\n",
        "                utter = batch[j]\n",
        "            \n",
        "                input_ids = [sp_id] + tokenizer.encode(utter)\n",
        "                input_hists.append(input_ids)\n",
        "\n",
        "                #Context just for debugging\n",
        "                context.append(utter)\n",
        "                \n",
        "                if len(input_hists) < window:\n",
        "                    continue\n",
        "                elif len(input_hists) > window:\n",
        "                    input_hists = input_hists[-window:]\n",
        "                    context = context[-window:]\n",
        "                \n",
        "                # Debugging\n",
        "                if i % 200 == 0:\n",
        "                    print()\n",
        "                    print('Context:')\n",
        "                    for c in context:\n",
        "                        print(c)\n",
        "                \n",
        "                # print('input_hists: ', input_hists)\n",
        "                # print('len input_hists: ', len(input_hists))\n",
        "\n",
        "                start_sp_id = input_hists[0][0]\n",
        "                next_sp_id = sp1_id if start_sp_id == sp2_id else sp2_id\n",
        "                assert start_sp_id != next_sp_id\n",
        "\n",
        "                input_ids = [bos_id] + list(chain.from_iterable(input_hists)) + [next_sp_id] #Because window is 5, so 6th utter is = sp2\n",
        "                # print('input_hists with bos: ', input_ids)\n",
        "                \n",
        "                token_type_ids = [[start_sp_id] * len(hist) if h % 2 == 0 else [next_sp_id] * len(hist) for h, hist in enumerate(input_hists)]\n",
        "                assert len(token_type_ids) == len(input_hists)\n",
        "                token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [next_sp_id]\n",
        "                # print('token_type_ids: ', token_type_ids)\n",
        "                assert len(input_ids) == len(token_type_ids)\n",
        "                input_len = len(input_ids)\n",
        "            \n",
        "                input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
        "                token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
        "                \n",
        "                # next_sp_id = sp2_id if input_hists[-1][0] == sp1_id else sp1_id\n",
        "\n",
        "                output_ids = nucleus_sampling(input_ids, token_type_ids, input_len, next_sp_id)                \n",
        "            # output_ids = self.model.generate(\n",
        "            #     input_ids=input_ids, token_type_ids=token_type_ids, pad_token_id=self.args.eos_id,\n",
        "            #     do_sample=True, top_p=self.args.top_p, max_length=self.args.max_len,\n",
        "            #     output_hidden_states=True, output_scores=True, return_dict_in_generate=True,\n",
        "            # ).sequences\n",
        "            # output_ids = output_ids[0].tolist()[input_len:]\n",
        "                res = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "                if j == len(batch) - 1:  # No subsequent sentence\n",
        "                    continue\n",
        "                \n",
        "                actual_res = batch[j+1]\n",
        "                \n",
        "                if i % 200 == 0:\n",
        "                    print()\n",
        "                    print(f\"Bot response: {res}\")\n",
        "                    print(f\"Actual response: {actual_res}\")\n",
        "\n",
        "                generated_responses.append(res)\n",
        "                actual_responses.append(actual_res)\n",
        "\n",
        "                # input_hists.append([next_sp_id] + tokenizer.encode(actual_res))\n",
        "                # context.append(actual_res)\n",
        "\n",
        "                # print('final input_hists: ', input_hists)    \n",
        "\n",
        "               \n",
        "    \n",
        "    return generated_responses, actual_responses\n",
        "\n",
        "            \n",
        "                \n",
        "def nucleus_sampling(input_ids, token_type_ids, input_len, next_sp_id):\n",
        "    output_ids = []\n",
        "    for pos in range(input_len, max_len):\n",
        "        output = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]  # (1, V)\n",
        "        output = F.softmax(output, dim=-1)  # (1, V)\n",
        "        \n",
        "        sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n",
        "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
        "        idx_remove = cumsum_probs > top_p\n",
        "        idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
        "        idx_remove[:, 0] = False\n",
        "        sorted_probs[idx_remove] = 0.0\n",
        "        sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n",
        "        \n",
        "        probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n",
        "        idx = torch.multinomial(probs, 1)  # (1, 1)\n",
        "        \n",
        "        idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
        "        output_ids.append(idx_item)\n",
        "        \n",
        "        if idx_item == eos_id:\n",
        "            break\n",
        "            \n",
        "        input_ids = torch.cat((input_ids, idx), dim=-1)\n",
        "        next_type_id = torch.LongTensor([[next_sp_id]]).to(device)\n",
        "        token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
        "        assert input_ids.shape == token_type_ids.shape\n",
        "        \n",
        "    return output_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_responses, actual_responses = infer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqa3p6oe4WpL",
        "outputId": "81a88b39-6c0c-4789-f149-657405090e21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/2540 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context:\n",
            "Yeah about 10 years ago I had a horrifying experience. It was 100% their fault but they hit the water barrels and survived. They had no injuries but they almost ran me off the road.\n",
            "Did you suffer any injuries?\n",
            "No I wasn't hit. It turned out they were drunk. I felt guilty but realized it was his fault.\n",
            "Why did you feel guilty? People really shouldn't drive drunk.\n",
            "I don't know I was new to driving and hadn't experienced anything like that. I felt like my horn made him swerve into the water barrels.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 999/2540 [01:40<01:21, 18.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context:\n",
            "I hate that people hit on others they know are married.\n",
            "Ugh, Why do people do that? It's annoying.\n",
            "Right? Happens to my husband all the time and it's just frustrating.\n",
            "Do you smack the women who do it?\n",
            "No, Since it's his associates at work!!.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 87%|████████▋ | 2200/2540 [03:04<00:40,  8.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context:\n",
            "I am going to play dominos tonight.\n",
            "That sounds like fun! Is it with friends?\n",
            "Well with some locals at the local brewpub.\n",
            "So cool, I hope it goes well.\n",
            "Yes I think I will do well even though I just learned last week how to play!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2540/2540 [03:30<00:00, 12.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(generated_responses) == len(actual_responses)\n",
        "print(len(generated_responses))\n",
        "print(len(actual_responses))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1mUOd7oel9c",
        "outputId": "217688ae-053d-4c9c-8f59-3b591be593c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "243\n",
            "243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store responses"
      ],
      "metadata": {
        "id": "TkRY1kwD4hai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "file_generated = \"\" + selected_model + \"_batch4_generated_responses_\" + dataset_name\n",
        "file_actual = \"\" + selected_model + \"_batch4_actual_responses_\"  + dataset_name\n",
        "\n",
        "with open(file_generated, \"wb\") as fp:\n",
        "    pickle.dump(generated_responses, fp)\n",
        "\n",
        "with open(file_actual, \"wb\") as fp:\n",
        "    pickle.dump(actual_responses, fp)\n",
        "\n",
        "files.download(file_generated) \n",
        "files.download(file_actual) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PwzgZpCaeio4",
        "outputId": "4868b184-0c94-4967-945d-995a23641d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c7db7e67-80c5-4057-9060-031021fd2942\", \"dialoGPT_batch4_generated_responses_empathetic_dialogues\", 14985)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_0614cf54-ada3-48b1-80b6-7ca1446553b2\", \"dialoGPT_batch4_actual_responses_empathetic_dialogues\", 16670)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading example\n",
        "# with open(\"gpt2_batch8_generated_responses\", \"rb\") as fp:   # Unpickling\n",
        "#     dummy = pickle.load(fp)"
      ],
      "metadata": {
        "id": "_z8JeEaC8Zta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute metrics"
      ],
      "metadata": {
        "id": "ObaRudgf5szZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bertscore = evaluate.load(\"bertscore\")\n",
        "chrf = evaluate.load(\"chrf\")"
      ],
      "metadata": {
        "id": "EOt5uCEb9DSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actual_responses = [[res] for res in actual_responses] #Refs must be in a list of list of str\n",
        "\n",
        "print(generated_responses[:5])\n",
        "print(actual_responses[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLDYzilo6UPC",
        "outputId": "6fa93934-8e5f-46a6-bf3d-20a4a5d6befc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"It's hard to believe the average person would've done that. I think it would've been pretty difficult to track down what's going on.\", 'Work and work with your boss for a change. I wish you the best in your future.', 'You must be so proud of yourself. I have a friend who has won his first, So I know it must have felt great when he did it!', 'I will, Thanks.', 'Oh. Well I hope you enjoyed it.']\n",
            "[[\"Wow, So your going to take being a bad person to the grave. Maybe you'll see her in the next life?\"], [\"Well I've been in the business all my life and have worked for some great people. So I pull from what I learned from them.\"], ['Oh, Wow. Not only to be able to do all the running but to view the scenery!'], ['I know.'], ['Sounds interesting!']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = sacrebleu.compute(predictions=generated_responses, references=actual_responses)\n",
        "\n",
        "rouge_score = rouge.compute(predictions=generated_responses, references=actual_responses)\n",
        "\n",
        "bert_score = bertscore.compute(predictions=generated_responses, references=actual_responses, lang='en')\n",
        "precision = bert_score['precision']\n",
        "recall = bert_score['recall']\n",
        "f1 = bert_score['f1']\n",
        "avg_precision_bert = sum(precision) / len(precision)\n",
        "avg_recall_bert = sum(recall) / len(recall)\n",
        "avg_f1_bert = sum(f1) / len(f1)\n",
        "\n",
        "chrf_score = chrf.compute(predictions=generated_responses, references=actual_responses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TR1Z3JL932Lb",
        "outputId": "88a93386-3296-4c6d-d075-4d339b47512e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Bleu score: \\n', bleu_score) #Range from 0 to 100\n",
        "print('Rouge score: \\n', rouge_score)\n",
        "print('Bert score: \\n', bert_score)\n",
        "print('Avg precision Bert score: ', avg_precision_bert)\n",
        "print('Avg recall Bert score: ', avg_recall_bert)\n",
        "print('Avg f1 Bert score: ', avg_f1_bert)\n",
        "print('chrf score: \\n', chrf_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu2u1KBSepFG",
        "outputId": "03aca546-9dbe-4c79-99ca-709d569f2626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bleu score: \n",
            " {'score': 0.8775860537020593, 'counts': [579, 39, 8, 4], 'totals': [3440, 3197, 2954, 2711], 'precisions': [16.83139534883721, 1.2198936502971536, 0.2708192281651997, 0.14754703061600885], 'bp': 0.9220971878521749, 'sys_len': 3440, 'ref_len': 3719}\n",
            "Rouge score: \n",
            " {'rouge1': 0.12720051415115013, 'rouge2': 0.012832279511119581, 'rougeL': 0.11058318450259344, 'rougeLsum': 0.11041933689210065}\n",
            "Bert score: \n",
            " {'precision': [0.8519506454467773, 0.8683867454528809, 0.837348222732544, 0.8608677983283997, 0.8518811464309692, 0.8643646836280823, 0.8481199741363525, 0.858927845954895, 0.8822838664054871, 0.8653178811073303, 0.8559589385986328, 0.8775131106376648, 0.8663605451583862, 0.8884487748146057, 0.9010105133056641, 0.8656351566314697, 0.8619168996810913, 0.872879147529602, 0.8998854756355286, 0.8512505292892456, 0.8912308216094971, 0.9395709037780762, 0.8470564484596252, 0.8360403180122375, 0.835598349571228, 0.8487783670425415, 0.8631832003593445, 0.8428822755813599, 0.8573969602584839, 0.8229183554649353, 0.8667437434196472, 0.8489456176757812, 0.8456289768218994, 0.8183607459068298, 0.8612685203552246, 0.8818740844726562, 0.8338316082954407, 0.8681842088699341, 0.8778812289237976, 0.8439117074012756, 0.8662999868392944, 0.8279998302459717, 0.8689056634902954, 0.8603638410568237, 0.8252323269844055, 0.8544784784317017, 0.8520952463150024, 0.8634689450263977, 0.8501836061477661, 0.8507073521614075, 0.8632632493972778, 0.8845298290252686, 0.8382782936096191, 0.8435182571411133, 0.8385956883430481, 0.8315532803535461, 0.8814510107040405, 0.880342960357666, 0.8468931317329407, 0.8394432067871094, 0.8669122457504272, 0.8725146055221558, 0.8362234830856323, 0.879485011100769, 0.8570245504379272, 0.8503152132034302, 0.8395375609397888, 0.8689098358154297, 0.8766155242919922, 0.8894447088241577, 0.8681371212005615, 0.9405916929244995, 0.8927969336509705, 0.856963038444519, 0.8486354351043701, 0.8908094763755798, 0.8391278982162476, 0.8551293015480042, 0.8387085795402527, 0.8391021490097046, 0.8541584014892578, 0.8500092625617981, 0.815274178981781, 0.8263087272644043, 0.8704919219017029, 0.8354367613792419, 0.8480538725852966, 0.8471850752830505, 0.8628846406936646, 0.8768534660339355, 0.8313261866569519, 0.8743367195129395, 0.8946464657783508, 0.8790638446807861, 0.8533576726913452, 0.8702597618103027, 0.8661661148071289, 0.8423084020614624, 0.8483582735061646, 0.9471336603164673, 0.8441643714904785, 0.8500336408615112, 0.90216064453125, 0.869580090045929, 0.8622646927833557, 0.8626784682273865, 0.8336952924728394, 0.8802851438522339, 0.8731487393379211, 0.8658113479614258, 0.8522194623947144, 0.8554202914237976, 0.8570350408554077, 0.8762760758399963, 0.8480762839317322, 0.8565701246261597, 0.860892117023468, 0.8812652826309204, 0.8363276720046997, 0.8458398580551147, 0.8411479592323303, 0.8724157214164734, 0.8540827035903931, 0.8692390322685242, 0.8918155431747437, 0.8819104433059692, 0.8678789138793945, 0.8877354860305786, 0.8455971479415894, 0.8929563760757446, 0.8518046736717224, 0.854498565196991, 0.8350290060043335, 0.8784918785095215, 0.8552023768424988, 0.8650240898132324, 0.9085104465484619, 0.8652865886688232, 0.8674424886703491, 0.8533326387405396, 0.8812468647956848, 0.8507592082023621, 0.8694057464599609, 0.8659560680389404, 0.8664711713790894, 0.8383197784423828, 0.8674277067184448, 0.8334276676177979, 0.8797158002853394, 0.8669518232345581, 0.8594643473625183, 0.8883548974990845, 0.8640281558036804, 0.8530158400535583, 0.8628860712051392, 0.8456698656082153, 0.8946188688278198, 0.9013601541519165, 0.8879563212394714, 0.8691644668579102, 0.9099798202514648, 0.8627839088439941, 0.8522188067436218, 0.8754966259002686, 0.8467620611190796, 0.8602622747421265, 0.833511233329773, 0.8553574085235596, 0.835343062877655, 0.8666182160377502, 0.8645230531692505, 0.8405035138130188, 0.8560935258865356, 0.8660211563110352, 0.912201464176178, 0.8503092527389526, 0.8691062927246094, 0.8250225782394409, 0.8878547549247742, 0.8491021394729614, 0.8693423867225647, 0.9200354218482971, 0.8705549240112305, 0.8685550093650818, 0.9256159663200378, 0.8477045297622681, 0.8567583560943604, 0.8141053915023804, 0.8861852288246155, 0.8537722229957581, 0.8921496868133545, 0.8662467002868652, 0.8522975444793701, 0.8789992332458496, 0.8634610176086426, 0.8777735829353333, 0.867789626121521, 0.8478967547416687, 0.8891083598136902, 0.8602296710014343, 0.8671482801437378, 0.8626127243041992, 0.8446216583251953, 0.8026734590530396, 0.871634840965271, 0.8768850564956665, 0.8319776058197021, 0.8756257891654968, 0.8380546569824219, 0.8694117069244385, 0.8892674446105957, 0.8880775570869446, 0.8388016223907471, 0.8555988669395447, 0.8498082756996155, 0.8671653270721436, 0.8651827573776245, 0.851407527923584, 0.8592717051506042, 0.8967159390449524, 0.8534121513366699, 0.8370914459228516, 0.8773974776268005, 0.87506103515625, 0.8665130734443665, 0.8819732069969177, 0.8488415479660034, 0.8611836433410645, 0.8708140850067139, 0.8663158416748047, 0.8288966417312622, 0.869332492351532, 0.8646743297576904, 0.8257465362548828, 0.8722483515739441, 0.8733034133911133, 0.8653861284255981, 0.8672015070915222, 0.8743430376052856, 0.8770856857299805, 0.8743209838867188, 0.848236620426178, 0.8756774663925171], 'recall': [0.8428590297698975, 0.8549922108650208, 0.8574894666671753, 0.8754485249519348, 0.8266229629516602, 0.8549561500549316, 0.8569132089614868, 0.8731718063354492, 0.8922079205513, 0.8594998121261597, 0.8561951518058777, 0.8405829668045044, 0.8604328036308289, 0.872014045715332, 0.8873276114463806, 0.8298748731613159, 0.8527016639709473, 0.8358445167541504, 0.8578271865844727, 0.8245251774787903, 0.867350697517395, 0.8693495392799377, 0.855719804763794, 0.8574244976043701, 0.8423447608947754, 0.9190292954444885, 0.8742820620536804, 0.8938416242599487, 0.8527634739875793, 0.8288395404815674, 0.8845471739768982, 0.8747045397758484, 0.8318714499473572, 0.8749005198478699, 0.8875195980072021, 0.8445795774459839, 0.8387331962585449, 0.8361536264419556, 0.8359243869781494, 0.8634936213493347, 0.8455244302749634, 0.8633427023887634, 0.8602713346481323, 0.8410913944244385, 0.8559713363647461, 0.8657307624816895, 0.8690752983093262, 0.8823298215866089, 0.8336233496665955, 0.8692449331283569, 0.8676456212997437, 0.8569289445877075, 0.9042443037033081, 0.858948290348053, 0.8372746706008911, 0.8433301448822021, 0.8683812022209167, 0.8381078243255615, 0.833001971244812, 0.8272014260292053, 0.863101601600647, 0.8847011923789978, 0.8170597553253174, 0.851976752281189, 0.8050819635391235, 0.8742135167121887, 0.8512840867042542, 0.8405485153198242, 0.8720472455024719, 0.8588697910308838, 0.870265543460846, 0.9578898549079895, 0.8180650472640991, 0.8651310205459595, 0.8555227518081665, 0.8255733847618103, 0.843765377998352, 0.8557036519050598, 0.8702520132064819, 0.8559000492095947, 0.8419507145881653, 0.8963837623596191, 0.8296368718147278, 0.8385854363441467, 0.8392089605331421, 0.8452833890914917, 0.8311640024185181, 0.8348777294158936, 0.8507882356643677, 0.8734477758407593, 0.8599918484687805, 0.8634788990020752, 0.8603205680847168, 0.8687883615493774, 0.8322323560714722, 0.901532769203186, 0.8484612703323364, 0.8792580366134644, 0.8194453716278076, 0.8918256163597107, 0.8455700278282166, 0.8340424299240112, 0.8747410774230957, 0.8317931890487671, 0.8433941602706909, 0.8709229826927185, 0.8536900281906128, 0.8584432005882263, 0.8478142023086548, 0.9021146893501282, 0.8462619781494141, 0.8641571402549744, 0.8537693619728088, 0.853946328163147, 0.868212878704071, 0.8380522727966309, 0.8517796993255615, 0.8584985733032227, 0.8800923824310303, 0.856052577495575, 0.841079831123352, 0.8614984750747681, 0.8418880701065063, 0.8308336138725281, 0.9069311022758484, 0.8462665677070618, 0.8445896506309509, 0.8704392910003662, 0.8485140204429626, 0.8266109228134155, 0.8806732296943665, 0.8477774262428284, 0.8121676445007324, 0.863961398601532, 0.8478161692619324, 0.8611338138580322, 0.8564934134483337, 0.8621506690979004, 0.8755694031715393, 0.8513189554214478, 0.8888168931007385, 0.8531956672668457, 0.8787533044815063, 0.8174795508384705, 0.8540692329406738, 0.8159204721450806, 0.8685861825942993, 0.8337201476097107, 0.8798960447311401, 0.8490159511566162, 0.853919267654419, 0.8918539881706238, 0.8535748720169067, 0.8440289497375488, 0.8228570222854614, 0.8176847100257874, 0.8794749975204468, 0.8792983889579773, 0.8764676451683044, 0.8685058355331421, 0.8678405284881592, 0.8654322028160095, 0.8422233462333679, 0.8660115003585815, 0.829704761505127, 0.8472498655319214, 0.8272206783294678, 0.8607752919197083, 0.8541581034660339, 0.8311326503753662, 0.8559361696243286, 0.8468983173370361, 0.8376369476318359, 0.8368909358978271, 0.852412223815918, 0.8758461475372314, 0.8421099781990051, 0.8386650085449219, 0.8399761915206909, 0.8419394493103027, 0.8587934374809265, 0.8399108052253723, 0.8675545454025269, 0.8567538261413574, 0.9733657240867615, 0.8891671895980835, 0.8588632345199585, 0.8670397400856018, 0.8511816263198853, 0.8491456508636475, 0.8628880381584167, 0.8680601119995117, 0.8674808144569397, 0.8780487775802612, 0.8706440329551697, 0.8651071786880493, 0.8470932245254517, 0.8576142191886902, 0.8774875998497009, 0.8576011657714844, 0.8532556891441345, 0.8582789301872253, 0.8599504828453064, 0.8461316823959351, 0.8420882821083069, 0.883310079574585, 0.8467795252799988, 0.8597128391265869, 0.8324205279350281, 0.8589986562728882, 0.8556239604949951, 0.8728511929512024, 0.8437730073928833, 0.8389856815338135, 0.8420882225036621, 0.8565793037414551, 0.859711229801178, 0.8344739675521851, 0.8412326574325562, 0.8677618503570557, 0.846017599105835, 0.8465918898582458, 0.8330824375152588, 0.8442766070365906, 0.8931159377098083, 0.8609697222709656, 0.8497129678726196, 0.8559849858283997, 0.8691750764846802, 0.846887469291687, 0.8144661784172058, 0.846325695514679, 0.82171630859375, 0.8712773323059082, 0.8633655309677124, 0.826980471611023, 0.8584798574447632, 0.8783311247825623, 0.8340203762054443, 0.8926326632499695, 0.881562352180481, 0.8298448920249939, 0.8610693216323853], 'f1': [0.8473804593086243, 0.8616374135017395, 0.8472991585731506, 0.8680968880653381, 0.8390620350837708, 0.8596346974372864, 0.852493941783905, 0.8659912943840027, 0.887218177318573, 0.8623989820480347, 0.8560770153999329, 0.8586511611938477, 0.8633865118026733, 0.8801547288894653, 0.8941166996955872, 0.8473779559135437, 0.8572845458984375, 0.8539605140686035, 0.8783531785011292, 0.8376746773719788, 0.8791285753250122, 0.9030972719192505, 0.8513661026954651, 0.84659743309021, 0.8389579653739929, 0.8825079798698425, 0.8686972260475159, 0.8676143288612366, 0.8550739288330078, 0.8258683085441589, 0.8755549192428589, 0.8616325855255127, 0.8386937975883484, 0.845686674118042, 0.8741970062255859, 0.862824022769928, 0.8362752795219421, 0.8518679738044739, 0.8563891649246216, 0.8535903692245483, 0.855786144733429, 0.8453019857406616, 0.8645669221878052, 0.8506184816360474, 0.840320885181427, 0.8600677847862244, 0.8605014681816101, 0.8727974891662598, 0.84182208776474, 0.8598762154579163, 0.8654488921165466, 0.8705106377601624, 0.870012640953064, 0.851163387298584, 0.8379346132278442, 0.8374003767967224, 0.8748673796653748, 0.8587063550949097, 0.8398900628089905, 0.8332774043083191, 0.8650026917457581, 0.8785656690597534, 0.8265305161476135, 0.8655123710632324, 0.8302416205406189, 0.862098753452301, 0.8453699946403503, 0.8544939160346985, 0.8743253946304321, 0.8738899230957031, 0.8692001104354858, 0.9491618871688843, 0.8537988662719727, 0.8610275983810425, 0.8520652055740356, 0.8569516539573669, 0.8414402604103088, 0.8554163575172424, 0.8541892170906067, 0.8474178910255432, 0.8480107188224792, 0.8725808262825012, 0.8223928213119507, 0.8324018716812134, 0.8545642495155334, 0.8403312563896179, 0.8395240306854248, 0.8409863710403442, 0.8567937612533569, 0.87514728307724, 0.8454161286354065, 0.8688738942146301, 0.8771478533744812, 0.8738959431648254, 0.8426626324653625, 0.8856202960014343, 0.8572222590446472, 0.8603866696357727, 0.8336512446403503, 0.9186479449272156, 0.8448665738105774, 0.8419620990753174, 0.888239324092865, 0.8502669334411621, 0.8527250289916992, 0.8667811155319214, 0.8435742259025574, 0.8692269921302795, 0.8602949380874634, 0.8835902810096741, 0.8492302894592285, 0.8597665429115295, 0.8553990721702576, 0.8649671077728271, 0.8580265045166016, 0.8472100496292114, 0.8563116788864136, 0.8697329759597778, 0.8576520681381226, 0.8509155511856079, 0.8411138653755188, 0.8669227957725525, 0.8479415774345398, 0.8496025204658508, 0.8993098139762878, 0.8637208938598633, 0.8560758829116821, 0.8790023326873779, 0.8470531105995178, 0.8585037589073181, 0.8659984469413757, 0.8511247634887695, 0.8234397172927856, 0.8711660504341125, 0.851493239402771, 0.863074541091919, 0.8817353844642639, 0.8637157678604126, 0.8714869618415833, 0.8523246049880981, 0.8850156664848328, 0.8519756197929382, 0.874054491519928, 0.8410198092460632, 0.8602254986763, 0.8269684910774231, 0.8680065870285034, 0.8335738778114319, 0.8798059225082397, 0.8578901290893555, 0.8566828370094299, 0.8901010751724243, 0.8587697148323059, 0.848498523235321, 0.8423963189125061, 0.8314418196678162, 0.886982262134552, 0.8901926279067993, 0.8821746110916138, 0.8688350319862366, 0.888410747051239, 0.8641059994697571, 0.8471916317939758, 0.8707282543182373, 0.8381466269493103, 0.8537064790725708, 0.8303540349006653, 0.8580578565597534, 0.8446457982063293, 0.8485046029090881, 0.8602081537246704, 0.8436888456344604, 0.8467646837234497, 0.85120689868927, 0.8812939524650574, 0.8628888130187988, 0.8553952574729919, 0.8317878246307373, 0.8632521033287048, 0.8455055952072144, 0.8640357255935669, 0.8781492114067078, 0.8690521717071533, 0.8626140356063843, 0.9488905072212219, 0.8679409623146057, 0.8578094840049744, 0.839739203453064, 0.8683307766914368, 0.851452648639679, 0.8772748708724976, 0.8671524524688721, 0.859822154045105, 0.8785237669944763, 0.8670375943183899, 0.8713943362236023, 0.8573165535926819, 0.8527278304100037, 0.8832597732543945, 0.8589133620262146, 0.8601459264755249, 0.8604403734207153, 0.8522170782089233, 0.8238298296928406, 0.8566067814826965, 0.880085825920105, 0.8393133282661438, 0.8675963282585144, 0.8352280259132385, 0.8641738295555115, 0.8721213340759277, 0.8803985118865967, 0.8412799835205078, 0.8472108244895935, 0.8459306955337524, 0.8618398308753967, 0.8624382615089417, 0.84285569190979, 0.8501564860343933, 0.8820013999938965, 0.8496987819671631, 0.8418148756027222, 0.8546658754348755, 0.8593931794166565, 0.8796133995056152, 0.8713448643684387, 0.8492770791053772, 0.8585764765739441, 0.8699938058853149, 0.8564915060997009, 0.8216180801391602, 0.857674777507782, 0.8426481485366821, 0.8479011058807373, 0.8677842617034912, 0.849510908126831, 0.8619191646575928, 0.8727308511734009, 0.8537058234214783, 0.8847908973693848, 0.8779267072677612, 0.8389399647712708, 0.8683120012283325], 'hashcode': 'roberta-large_L17_no-idf_version=0.3.12(hug_trans=4.30.1)'}\n",
            "Avg precision Bert score:  0.8626082144646978\n",
            "Avg recall Bert score:  0.8561797642413481\n",
            "Avg f1 Bert score:  0.8592071641129231\n",
            "chrf score: \n",
            " {'score': 13.28703735774083, 'char_order': 6, 'word_order': 0, 'beta': 2}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Play an audio beep. Any audio URL will do.\n",
        "from google.colab import output\n",
        "output.eval_js('new Audio(\"https://upload.wikimedia.org/wikipedia/commons/0/05/Beep-09.ogg\").play()')"
      ],
      "metadata": {
        "id": "5xD5xEY6Bjs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
        "# references = [[\"hello there general kenobi\", \"hello there !\"],\n",
        "#                  [\"foo bar foobar\", \"foo bar foobar\"]]\n",
        "# sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "# results = sacrebleu.compute(predictions=predictions, \n",
        "#                              references=references)\n",
        "# print(results)\n",
        "\n",
        "# results = rouge.compute(predictions=predictions, \n",
        "#                              references=references)\n",
        "# print(results)\n",
        "\n",
        "# results = bertscore.compute(predictions=predictions, \n",
        "#                              references=references, lang='eng')\n",
        "# print(results)\n",
        "\n",
        "# results = chrf.compute(predictions=predictions, \n",
        "#                              references=references)\n",
        "# print(results)"
      ],
      "metadata": {
        "id": "_j6wrwLN9YTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('/content/gpt2_epoch6_daily_dialog.ckpt') \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "xpMHNRTfI7u9",
        "outputId": "e155cf64-577a-42d7-f629-a6ea9eb2819a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_889845ea-a03b-4f02-ac0f-f3ab2f061ae2\", \"gpt2_epoch6_daily_dialog.ckpt\", 1493502169)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f664fd67577746e08255ecbdce4d7414": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_568c416462e94684978348f5221d452c",
              "IPY_MODEL_edb5a3f280a243019cffc86268a70226",
              "IPY_MODEL_7a3a3a1274504f86a1197b0431d2f1a9"
            ],
            "layout": "IPY_MODEL_f3901da562094eab903947ab3932e78e"
          }
        },
        "568c416462e94684978348f5221d452c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7f7f9e86afc487bad4225675751c2d9",
            "placeholder": "​",
            "style": "IPY_MODEL_941c2437df2642358d2ae8346dd29fe1",
            "value": "100%"
          }
        },
        "edb5a3f280a243019cffc86268a70226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f33d90c071ef472d88da1f82e99b2767",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c537cd567f6c4004852d89f2d958b13b",
            "value": 3
          }
        },
        "7a3a3a1274504f86a1197b0431d2f1a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e54e40923eb40bfa198cfa51a61118c",
            "placeholder": "​",
            "style": "IPY_MODEL_5d7bad610ef64a14889724b74af36d65",
            "value": " 3/3 [00:00&lt;00:00, 149.52it/s]"
          }
        },
        "f3901da562094eab903947ab3932e78e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7f7f9e86afc487bad4225675751c2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941c2437df2642358d2ae8346dd29fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f33d90c071ef472d88da1f82e99b2767": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c537cd567f6c4004852d89f2d958b13b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9e54e40923eb40bfa198cfa51a61118c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d7bad610ef64a14889724b74af36d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}