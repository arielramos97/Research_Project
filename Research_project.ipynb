{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, get_polynomial_decay_schedule_with_warmup\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import chain\n",
    "import torch\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os, sys\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (C:/Users/aramosvela/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "Found cached dataset daily_dialog (C:/Users/aramosvela/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "Found cached dataset daily_dialog (C:/Users/aramosvela/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset('daily_dialog.py', split='train')\n",
    "validation_dataset = load_dataset('daily_dialog.py', split='validation')\n",
    "test_dataset = load_dataset('daily_dialog.py', split='test')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = 'Ġ'\n",
    "pre_quote = '’'\n",
    "end_marks = ['.', ',', '?', '!', '...']\n",
    "quotes = ['\"', '\\'']\n",
    "abbreviations = ['s', 'd', 't', 'm', 're', 'll', 've', 'S', 'D', 'T', 'M', 'Re', 'Ll', 'Ve']\n",
    "\n",
    "def process_token_list(token_list):\n",
    "    token_list[0] = token_list[0].capitalize()\n",
    "    \n",
    "    quote_count = 0\n",
    "    for i, token in enumerate(token_list):\n",
    "        if space in token:\n",
    "            if token[1:] in end_marks or token[1:] in abbreviations:\n",
    "                token_list[i] = token[1:]\n",
    "                \n",
    "            if token[1:] == quotes[1]:\n",
    "                if i<len(token_list)-1:\n",
    "                    if token_list[i+1] in abbreviations or (token_list[i+1][0] == space and token_list[i+1][1:] in abbreviations):\n",
    "                        token_list[i] = token[1:]\n",
    "                        \n",
    "        if token[0] == space and token[1:] in quotes:\n",
    "            if quote_count % 2 == 1:\n",
    "                token_list[i] = token[1:]\n",
    "                quote_count = 0\n",
    "            else:\n",
    "                if i<len(token_list)-1 and token_list[i+1][0] == space:\n",
    "                    token_list[i+1] = token_list[i+1][1:]\n",
    "                quote_count += 1\n",
    "                \n",
    "        if token in end_marks or token[1:] in end_marks:\n",
    "            if i<len(token_list)-1:\n",
    "                if token_list[i+1][0] != space:\n",
    "                    token_list[i+1] = space + token_list[i+1].capitalize()\n",
    "                else:\n",
    "                    token_list[i+1] = space + token_list[i+1][1:].capitalize()\n",
    "                \n",
    "    new_token_list = [token for token in token_list if token != space and len(token)>0]\n",
    "    if new_token_list[-1] not in end_marks:\n",
    "        new_token_list.append(end_marks[0])\n",
    "        \n",
    "    return new_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_daily(tokenizer, train_frac=0.85):\n",
    "    dataset = load_dataset('daily_dialog')\n",
    "    train_dialogues = dataset['train']['dialog']\n",
    "    valid_dialogues = dataset['validation']['dialog']\n",
    "    test_dialogues = dataset['test']['dialog']\n",
    "    \n",
    "    total_dialogues = train_dialogues + valid_dialogues + test_dialogues\n",
    "    \n",
    "    for i, dialogue in enumerate(tqdm(total_dialogues)):\n",
    "        new_dialogue = []\n",
    "        for utter in dialogue:\n",
    "            token_list = tokenizer.tokenize(utter.strip().replace(pre_quote, quotes[1]))\n",
    "            token_list = process_token_list(token_list)\n",
    "            text = tokenizer.convert_tokens_to_string(token_list)\n",
    "            new_dialogue.append(text)\n",
    "            \n",
    "        total_dialogues[i] = new_dialogue\n",
    "    \n",
    "    train_utter_num = 0\n",
    "    valid_utter_num = 0\n",
    "    train_dialogues = total_dialogues[:int(len(total_dialogues)*train_frac)]\n",
    "    valid_dialogues = total_dialogues[int(len(total_dialogues)*train_frac):]\n",
    "    \n",
    "    for dialogue in train_dialogues:\n",
    "        train_utter_num += len(dialogue)\n",
    "        \n",
    "    for dialogue in valid_dialogues:\n",
    "        valid_utter_num += len(dialogue)\n",
    "    \n",
    "    return train_dialogues, valid_dialogues, train_utter_num, valid_utter_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset daily_dialog (C:/Users/aramosvela/.cache/huggingface/datasets/daily_dialog/default/1.0.0/1d0a58c7f2a4dab5ed9d01dbde8e55e0058e589ab81fce5c2df929ea810eabcd)\n",
      "100%|██████████| 3/3 [00:00<00:00, 428.53it/s]\n",
      "100%|██████████| 13118/13118 [00:08<00:00, 1540.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train dialogues: 11150\n",
      "The number of valid dialogues: 1968\n",
      "The number of train utterances: 87467\n",
      "The number of valid utterances: 15512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dialogues = []\n",
    "valid_dialogues = []\n",
    "num_train = 0\n",
    "num_valid = 0\n",
    "\n",
    "part_train_dialogues, part_valid_dialogues, part_num_train, part_num_valid = load_daily(tokenizer)\n",
    "   \n",
    "train_dialogues += part_train_dialogues\n",
    "valid_dialogues += part_valid_dialogues\n",
    "\n",
    "# print(\"#\"*50 + f\"Analysis on {data_name}\" + \"#\"*50)\n",
    "print(f\"The number of train dialogues: {len(part_train_dialogues)}\")\n",
    "print(f\"The number of valid dialogues: {len(part_valid_dialogues)}\")    \n",
    "print(f\"The number of train utterances: {part_num_train}\")    \n",
    "print(f\"The number of valid utterances: {part_num_valid}\")\n",
    "\n",
    "num_train += part_num_train\n",
    "num_valid += part_num_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11150/11150 [00:05<00:00, 1909.58it/s]\n",
      "100%|██████████| 1968/1968 [00:01<00:00, 1960.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extrac ids (input_ids, token_ids) from processed text\n",
    "\n",
    "def extract_ids(dialogues):\n",
    "\n",
    "    ids = []\n",
    "    for dialogue in tqdm(dialogues):\n",
    "            dialogue_ids = []\n",
    "            for utter in dialogue:\n",
    "                tokens = tokenizer.tokenize(utter)\n",
    "                token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "                dialogue_ids.append(token_ids)\n",
    "            ids.append(dialogue_ids)\n",
    "            \n",
    "    assert len(ids) == len(dialogues)\n",
    "\n",
    "    return ids  \n",
    "\n",
    "train_ids = extract_ids(train_dialogues)\n",
    "valid_ids = extract_ids(valid_dialogues) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "sp1_token = '<sp1>'\n",
    "sp2_token = '<sp2>'\n",
    "bos_token = '<bos>'\n",
    "max_turns = 5\n",
    "max_len = 1024\n",
    "seed = 0\n",
    "gpu = 0\n",
    "\n",
    "#Tokeniser\n",
    "special_tokens = {'bos_token': bos_token,\n",
    "                'additional_special_tokens': [sp1_token, sp2_token]}\n",
    "\n",
    "eos_token = tokenizer.eos_token\n",
    "num_new_tokens = tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "vocab_size = len(vocab)\n",
    "bos_id = vocab[bos_token]\n",
    "eos_id = vocab[eos_token]\n",
    "sp1_id = vocab[sp1_token]\n",
    "sp2_id = vocab[sp2_token]\n",
    "\n",
    "lr = 2e-5\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "num_epochs = 10\n",
    "warmup_ratio = 0.1\n",
    "last_epoch = 0\n",
    "end_command = 'Abort!'\n",
    "top_p = 0.8\n",
    "\n",
    "\n",
    "ckpt_dir = 'saved_models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dials):\n",
    "\n",
    "        self.input_ids = []  # (N, L)\n",
    "        self.token_type_ids = []  # (N, L)\n",
    "        self.labels = []  # (N, L)\n",
    "            \n",
    "        for dial in tqdm(dials):\n",
    "            hists = []\n",
    "            for u, utter in enumerate(dial):\n",
    "                if u % 2 == 0:\n",
    "                    hists.append([sp1_id] + utter)\n",
    "                else:\n",
    "                    hists.append([sp2_id] + utter)\n",
    "                    \n",
    "            for h in range(len(hists)):\n",
    "                if hists[h][0] == sp2_id:\n",
    "                    start = max(0, h - max_turns+1)\n",
    "                    for s in range(start, h):\n",
    "                        contexts = hists[s:h+1]\n",
    "                        input_ids = [bos_id] + list(chain.from_iterable(contexts)) + [eos_id]\n",
    "                        if len(input_ids) <= max_len:\n",
    "                            start_sp_id, next_sp_id = contexts[0][0], contexts[1][0]\n",
    "                            token_type_ids = [[start_sp_id] * len(ctx) if c % 2 == 0 else [next_sp_id] * len(ctx) for c, ctx in enumerate(contexts)]\n",
    "                            assert token_type_ids[-1][0] == sp2_id\n",
    "                            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [sp2_id]\n",
    "                            assert len(input_ids) == len(token_type_ids)\n",
    "                            \n",
    "                            labels = [[-100] * len(ctx) if c < len(contexts)-1 else [-100] + ctx[1:] for c, ctx in enumerate(contexts)]\n",
    "                            assert labels[-1][1:] == contexts[-1][1:]\n",
    "                            labels = [-100] + list(chain.from_iterable(labels)) + [eos_id]\n",
    "                            assert len(input_ids) == len(labels)\n",
    "                            \n",
    "                            self.input_ids.append(input_ids)\n",
    "                            self.token_type_ids.append(token_type_ids)\n",
    "                            self.labels.append(labels)\n",
    "                            \n",
    "                            break\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.token_type_ids[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "class PadCollate():\n",
    "    def __init__(self, eos_id):\n",
    "        self.eos_id = eos_id\n",
    "        \n",
    "    def pad_collate(self, batch):\n",
    "        input_ids, token_type_ids, labels =[], [], []\n",
    "        for idx, seqs in enumerate(batch):\n",
    "            input_ids.append(torch.LongTensor(seqs[0]))\n",
    "            token_type_ids.append(torch.LongTensor(seqs[1]))\n",
    "            labels.append(torch.LongTensor(seqs[2]))\n",
    "            \n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.eos_id)\n",
    "        token_type_ids = torch.nn.utils.rnn.pad_sequence(token_type_ids, batch_first=True, padding_value=self.eos_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "    \n",
    "        return input_ids, token_type_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the model...\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{gpu}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Loading the model...\")\n",
    "fix_seed(seed)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.resize_token_embeddings(vocab_size)\n",
    "\n",
    "max_len = min(max_len, model.config.n_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the optimizer...\n",
      "Loading train & valid data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11150/11150 [00:00<00:00, 15379.39it/s]\n",
      "100%|██████████| 1968/1968 [00:00<00:00, 23428.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load optimizer\n",
    "print(\"Loading the optimizer...\")\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "best_loss = sys.float_info.max\n",
    "\n",
    "# Load train & valid dataset\n",
    "print(\"Loading train & valid data...\")\n",
    "\n",
    "train_set = CustomDataset(train_ids)\n",
    "valid_set = CustomDataset(valid_ids)\n",
    "\n",
    "ppd = PadCollate(eos_id=eos_id)\n",
    "\n",
    "train_loader = DataLoader(train_set, \n",
    "                            collate_fn=ppd.pad_collate, \n",
    "                            shuffle=True, \n",
    "                            batch_size=batch_size, \n",
    "                            num_workers=num_workers, \n",
    "                            pin_memory=True)\n",
    "\n",
    "valid_loader = DataLoader(valid_set, \n",
    "                            collate_fn=ppd.pad_collate,\n",
    "                            batch_size=batch_size, \n",
    "                            num_workers=num_workers, \n",
    "                            pin_memory=True)\n",
    "    \n",
    "# Calculate total training steps\n",
    "num_batches = len(train_loader)\n",
    "total_train_steps = num_epochs * num_batches\n",
    "warmup_steps = int(warmup_ratio * total_train_steps)\n",
    "\n",
    "sched = get_polynomial_decay_schedule_with_warmup(\n",
    "    optim,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_train_steps,\n",
    "    power=2\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation():\n",
    "\n",
    "    print(\"Validation processing...\")\n",
    "    model.eval()\n",
    "            \n",
    "    valid_losses = []\n",
    "    valid_ppls = []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(valid_loader)):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            input_ids, token_type_ids, labels = \\\n",
    "                input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids = token_type_ids,\n",
    "                labels = labels\n",
    "            )\n",
    "            \n",
    "            loss, logits = outputs[0], outputs[1]\n",
    "            \n",
    "            valid_losses.append(loss.detach())\n",
    "            ppl = torch.exp(loss.detach())\n",
    "            valid_ppls.append(ppl)\n",
    "        \n",
    "        valid_losses = [loss.item() for loss in valid_losses]\n",
    "        valid_ppls = [ppl.item() if not math.isinf(ppl.item()) else 1e+8 for ppl in valid_ppls]\n",
    "        valid_loss = np.mean(valid_losses)\n",
    "        valid_ppl = np.mean(valid_ppls)\n",
    "        \n",
    "        if math.isnan(valid_ppl):\n",
    "            valid_ppl = 1e+8\n",
    "            \n",
    "    return valid_loss, valid_ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    fix_seed(seed)  # Fix seed before training\n",
    "    print(\"Training starts.\")\n",
    "\n",
    "    last_epoch= 0\n",
    "    \n",
    "    start_epoch = last_epoch +1\n",
    "    for epoch in range(start_epoch, start_epoch+num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        print(f\"#\"*50 + f\"Epoch: {epoch}\" + \"#\"*50)\n",
    "        train_losses = []\n",
    "        train_ppls = []\n",
    "        for i, batch in enumerate(tqdm(train_loader)):\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            input_ids, token_type_ids, labels = \\\n",
    "                input_ids.to(device), token_type_ids.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                token_type_ids = token_type_ids,\n",
    "                labels = labels\n",
    "            )\n",
    "            \n",
    "            loss, logits = outputs[0], outputs[1]\n",
    "            \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            sched.step()\n",
    "            \n",
    "            train_losses.append(loss.detach())\n",
    "            ppl = torch.exp(loss.detach())\n",
    "            train_ppls.append(ppl)\n",
    "        \n",
    "        train_losses = [loss.item() for loss in train_losses]\n",
    "        train_ppls = [ppl.item() if not math.isinf(ppl.item()) else 1e+8 for ppl in train_ppls]\n",
    "        train_loss = np.mean(train_losses)\n",
    "        train_ppl = np.mean(train_ppls)\n",
    "        print(f\"Train loss: {train_loss} || Train perplexity: {train_ppl}\")\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"PPL/train\", train_ppl, epoch)\n",
    "        \n",
    "        last_epoch += 1\n",
    "        \n",
    "        valid_loss, valid_ppl = validation()\n",
    "            \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            state_dict = {\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optim_state_dict': optim.state_dict(),\n",
    "                'sched_state_dict': sched.state_dict(),\n",
    "                'loss': best_loss,\n",
    "                'epoch': last_epoch\n",
    "            }\n",
    "            \n",
    "            torch.save(state_dict, f\"{ckpt_dir}/best_ckpt_epoch={epoch}_valid_loss={round(best_loss, 4)}.ckpt\")\n",
    "            print(\"*\"*10 + \"Current best checkpoint is saved.\" + \"*\"*10)\n",
    "            print(f\"{ckpt_dir}/best_ckpt_epoch={epoch}_valid_loss={round(best_loss, 4)}.ckpt\")\n",
    "            \n",
    "        print(f\"Best valid loss: {best_loss}\")\n",
    "        print(f\"Valid loss: {valid_loss} || Valid perplexity: {valid_ppl}\")\n",
    "        \n",
    "        writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "        writer.add_scalar(\"PPL/valid\", valid_ppl, epoch)\n",
    "        \n",
    "        writer.add_scalars(\"Losses\", {\n",
    "            'train': train_loss, \n",
    "            'valid': valid_loss,\n",
    "        }, epoch)\n",
    "        writer.add_scalars(\"PPLs\", {\n",
    "            'train': train_ppl,\n",
    "            'valid': valid_ppl,\n",
    "        }, epoch)\n",
    "            \n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training starts.\n",
      "##################################################Epoch: 1##################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 665/5223 [1:19:47<9:06:53,  7.20s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[59], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m input_ids, token_type_ids, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     17\u001b[0m input_ids, token_type_ids, labels \u001b[39m=\u001b[39m \\\n\u001b[0;32m     18\u001b[0m     input_ids\u001b[39m.\u001b[39mto(device), token_type_ids\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 20\u001b[0m outputs \u001b[39m=\u001b[39m model(\n\u001b[0;32m     21\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m     22\u001b[0m     token_type_ids \u001b[39m=\u001b[39;49m token_type_ids,\n\u001b[0;32m     23\u001b[0m     labels \u001b[39m=\u001b[39;49m labels\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m loss, logits \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m], outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m     28\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1075\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   1072\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1073\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1075\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[0;32m   1076\u001b[0m     input_ids,\n\u001b[0;32m   1077\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1078\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1079\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1080\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1081\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1082\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1083\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1084\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1085\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1086\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1087\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1088\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1089\u001b[0m )\n\u001b[0;32m   1090\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1092\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:899\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    889\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    890\u001b[0m         create_custom_forward(block),\n\u001b[0;32m    891\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    896\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    897\u001b[0m     )\n\u001b[0;32m    898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 899\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[0;32m    900\u001b[0m         hidden_states,\n\u001b[0;32m    901\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    902\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    903\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[0;32m    904\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    905\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    906\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    907\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    908\u001b[0m     )\n\u001b[0;32m    910\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    911\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:389\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    387\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[0;32m    388\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[1;32m--> 389\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[0;32m    390\u001b[0m     hidden_states,\n\u001b[0;32m    391\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[0;32m    392\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m    393\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m    394\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m    395\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    396\u001b[0m )\n\u001b[0;32m    397\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[0;32m    398\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:332\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m--> 332\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_heads(attn_output, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim)\n\u001b[0;32m    333\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(attn_output)\n\u001b[0;32m    334\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(attn_output)\n",
      "File \u001b[1;32mc:\\Users\\aramosvela\\Documents\\Research_Project\\research_env\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:285\u001b[0m, in \u001b[0;36mGPT2Attention._merge_heads\u001b[1;34m(self, tensor, num_heads, attn_head_size)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_merge_heads\u001b[39m(\u001b[39mself\u001b[39m, tensor, num_heads, attn_head_size):\n\u001b[0;32m    282\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[39m    Merges attn_head_size dim and num_attn_heads dim into hidden_size\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     tensor \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\n\u001b[0;32m    286\u001b[0m     new_shape \u001b[39m=\u001b[39m tensor\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m (num_heads \u001b[39m*\u001b[39m attn_head_size,)\n\u001b[0;32m    287\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mview(new_shape)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer():\n",
    "    print(\"Let's start!\")\n",
    "    print(f\"If you want to quit the conversation, please type \\\"{end_command}\\\".\")\n",
    "    model.eval()\n",
    "    fix_seed(seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_hists = []\n",
    "        \n",
    "        while True:\n",
    "            utter = input(\"You: \")\n",
    "            if utter == end_command:\n",
    "                print(\"Bot: Good bye.\")\n",
    "                break\n",
    "            \n",
    "            input_ids = [sp1_id] + tokenizer.encode(utter)\n",
    "            input_hists.append(input_ids)\n",
    "            \n",
    "            if len(input_hists) >= max_turns:\n",
    "                num_exceeded = len(input_hists) - max_turns + 1\n",
    "                input_hists = input_hists[num_exceeded:]\n",
    "                \n",
    "            input_ids = [bos_id] + list(chain.from_iterable(input_hists)) + [sp2_id]\n",
    "            start_sp_id = input_hists[0][0]\n",
    "            next_sp_id = sp1_id if start_sp_id == sp2_id else sp2_id\n",
    "\n",
    "            assert start_sp_id != next_sp_id\n",
    "            token_type_ids = [[start_sp_id] * len(hist) if h % 2 == 0 else [next_sp_id] * len(hist) for h, hist in enumerate(input_hists)]\n",
    "            assert len(token_type_ids) == len(input_hists)\n",
    "            token_type_ids = [start_sp_id] + list(chain.from_iterable(token_type_ids)) + [sp2_id]\n",
    "            assert len(input_ids) == len(token_type_ids)\n",
    "            input_len = len(input_ids)\n",
    "            \n",
    "            input_ids = torch.LongTensor(input_ids).unsqueeze(0).to(device)\n",
    "            token_type_ids = torch.LongTensor(token_type_ids).unsqueeze(0).to(device)\n",
    "            \n",
    "            output_ids = nucleus_sampling(input_ids, token_type_ids, input_len)                \n",
    "            # output_ids = self.model.generate(\n",
    "            #     input_ids=input_ids, token_type_ids=token_type_ids, pad_token_id=self.args.eos_id,\n",
    "            #     do_sample=True, top_p=self.args.top_p, max_length=self.args.max_len,\n",
    "            #     output_hidden_states=True, output_scores=True, return_dict_in_generate=True,\n",
    "            # ).sequences\n",
    "            # output_ids = output_ids[0].tolist()[input_len:]\n",
    "            res = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "            \n",
    "            print(f\"Bot: {res}\")\n",
    "            input_hists.append([sp2_id] + tokenizer.encode(res))\n",
    "                \n",
    "def nucleus_sampling(input_ids, token_type_ids, input_len):\n",
    "    output_ids = []\n",
    "    for pos in range(input_len, max_len):\n",
    "        output = model(input_ids=input_ids, token_type_ids=token_type_ids)[0][:, pos-1]  # (1, V)\n",
    "        output = F.softmax(output, dim=-1)  # (1, V)\n",
    "        \n",
    "        sorted_probs, sorted_idxs = torch.sort(output, descending=True)\n",
    "        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)  # (1, V)\n",
    "        idx_remove = cumsum_probs > top_p\n",
    "        idx_remove[:, 1:] = idx_remove[:, :-1].clone()\n",
    "        idx_remove[:, 0] = False\n",
    "        sorted_probs[idx_remove] = 0.0\n",
    "        sorted_probs /= torch.sum(sorted_probs, dim=-1, keepdim=True)  # (1, V)\n",
    "        \n",
    "        probs = torch.zeros(output.shape, device=device).scatter_(-1, sorted_idxs, sorted_probs)  # (1, V)\n",
    "        idx = torch.multinomial(probs, 1)  # (1, 1)\n",
    "        \n",
    "        idx_item = idx.squeeze(-1).squeeze(-1).item()\n",
    "        output_ids.append(idx_item)\n",
    "        \n",
    "        if idx_item == eos_id:\n",
    "            break\n",
    "            \n",
    "        input_ids = torch.cat((input_ids, idx), dim=-1)\n",
    "        next_type_id = torch.LongTensor([[sp2_id]]).to(device)\n",
    "        token_type_ids = torch.cat((token_type_ids, next_type_id), dim=-1)\n",
    "        assert input_ids.shape == token_type_ids.shape\n",
    "        \n",
    "    return output_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
